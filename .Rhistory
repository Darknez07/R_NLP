read.table("javalist.txt", sep='@', header = False)
read.table("javalist.txt", sep='@', header = false)
read.table("javalist.txt", sep='@', header = F)
df<-read.table("javalist.txt", sep='@', header = F)
df.colnames
df.colnames()
names(df)
df
names(x)<-c("Name","Version"_
names(x)<-c("Name","Version")
names(df)<-c("Name","Version")
df
library(dplyr)
select()
select(df)
select(df,"Version")
select(df,"Version" == 11.0.7)
select(df,"Version" == "11.0.7")
select(df,"Name")
airquality
read.table("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip", sep=',')
read.table("www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip", sep=',')
read.table("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip", sep=',')
read.table("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip")
read.csv("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip")
readRDS("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip")
readRDS(unz("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip","chicago.rds"))
unz
readRDS(unz("http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip","chicago.rds"))
pwd
cwd()
cwd
pwd()
getwd()
chicago<-readRDS("chicago.rds")
chicago$city
glimpse(chicago)
dim(chicago)]
dim(chicago)
select(chicago,city:dptp
)
subset<-select(chicago,city:dptp)
head(subset)
?head
head(subset, n = 5L)
names(subset)
subset<-select(chicago,-(city:dptp))
dim(subset)\
dim(subset)
head(subset)
df<-select(chicago, ends_with("2"))
glimpse(df)
df<-select(chicago,starts_with("d"))
glimpse(df)
f<-filter(chicago, pm25tmean2 > 30)
f
glimpse(chic.f)
glimpse(f)
unique.data.frame(chicago$city)
unique.data.frame(chicago$city)
unique.data.frame(chicago)
unique.data.frame(chicago)$city
summary(f)
summary(f$tmpd)
chic.f<-filter(chicago, pm25tmean2 > 30 & temp > 80)
chic.f<-filter(chicago, pm25tmean2 > 30 & tmpd > 80)
chic.f
apply(chic.f$date, class)
apply(chic.f$date,class())
apply(classmchic.f$date)
apply(class, chic.f$date)
sapply(chic.f$date,class)
arrange(chicago, date)
head(arrange(chicago, date))
filter(arrange(chicago, date), pm25team2 != NA)
filter(arrange(chicago, date), pm25tmean2 != NA)
filter(arrange(chicago, date), pm25tmean2)
filter(arrange(chicago, date), pm25tmean2 == NA)
filter(arrange(chicago, date), pm25tmean2 != "NA")
glimpse(filter(arrange(chicago, date), pm25tmean2 != "NA"))
notnachicago<-filter(arrange(chicago, date), pm25tmean2 != "NA")
tail(select(notnachicago,date:pm25tmean2),5)
tail(arrange(chicago, desc(date)),5)
notnachicago <- rename(notnachicago, dewpoint = dptp, pm25 = pm25tmean2)
head(notnachicago,5)
head(notnachicago[:, 1:5],5)
head(notnachicago[, 1:5],5)
notnachicago<- notnachicago %>% mutate(pm25diff = pm25 - mean(pm25))
notnachicago
head(notnachicago)
fr<-filter(notnachicago, pm25diff < 0)
fr
glimpse(fr)
fr<-select(filter(notnachicago, pm25diff < 0), pm25diff)
fr
glimpse(fr)
fr<-select(filter(notnachicago, pm25diff < 0), date:pm25diff)
head(fr,5)
fr<-select(filter(notnachicago, pm25diff < 0),date:pm25diff)
head(fr,5)
head(fr)
fr<-select(filter(notnachicago, pm25diff < 0), pm25diff)
fr<-select(filter(notnachicago, pm25diff < 0), c(date,pm25diff))
head(fr)
fr<-arrange(fr,date)
head(fr)
fr<-arrange(fr,pm25diff)
head(fr)
notnachicago<- notnachicago %>% transmute(pm25diff = pm25 - mean(pm25),o3trend=o3tmean2 - mean(o3tmean2))
head(nonatchicago)
notnachicago<- notnachicago %>% transmute(pm25diff = pm25 - mean(pm25),o3trend=o3tmean2 - mean(o3tmean2))
notnachicago<-filter(arrange(chicago, date), pm25tmean2 != "NA")
head(transmute(notnachicago,pm25diff = pm25 - mean(pm25),o3trend=o3tmean2 - mean(o3tmean2)))
notnachicago <- rename(notnachicago, dewpoint = dptp, pm25 = pm25tmean2)
head(transmute(notnachicago,pm25diff = pm25 - mean(pm25),o3trend=o3tmean2 - mean(o3tmean2)))
notnachicago<- notnachicago %>% mutate(year = as.POSIXlt(date)$year + 1900)
head(notnachicago(
)
)
head(notnachicago)
notnachicago %>% mutate(onlyyear = as.POSIXlt(date)$year)
notnachicago %>% head(mutate(onlyyear = as.POSIXlt(date)$year))
head(notnachicago %>% mutate(onlyyear = as.POSIXlt(date)$year))
years<- group_by(notnachicago, year)
head(years)
tail(years)
q()
fr
notnachicago
q()
setwd("/media/skanzz/Linux2/Machine Learning A-Z Template Folder/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing/R")
library(tm)
library(SnowballC)
library(textstem)
?install.koRpus.lang
available.koRpus.lang()
# Import dataset
dataset = read.delim('Restaurant_Reviews.tsv', quote = "",
stringsAsFactors = FALSE )
# Construct the corpus
corpus = VCorpus(VectorSource(dataset$Review))
# Putting all in lower case
# Each of the words should be unique
corpus = tm_map(corpus, content_transformer((tolower)))
# Remove numbers from the data
# Indeed helpful
corpus = tm_map(corpus, removeNumbers)
## Remove puntuation
corpus = tm_map(corpus, removePunctuation)
## Remove stopwords
corpus = tm_map(corpus, removeWords, stpword)
# Here is stemmed words
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, lemmatize_words)
corpus = tm_map(corpus, stripWhitespace)
# Creating bag of words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.9)
dtm
View(dtm)
# Import dataset
dataset = read.delim('Restaurant_Reviews.tsv', quote = "",
stringsAsFactors = FALSE )
# Cleaning the texts
# install.packages('tm')
library(tm)
library(SnowballC)
library(textstem)
# Construct the corpus
corpus = VCorpus(VectorSource(dataset$Review))
# Putting all in lower case
# Each of the words should be unique
corpus = tm_map(corpus, content_transformer((tolower)))
# Remove numbers from the data
# Indeed helpful
corpus = tm_map(corpus, removeNumbers)
## Remove puntuation
corpus = tm_map(corpus, removePunctuation)
## Remove stopwords
corpus = tm_map(corpus, removeWords, stpword)
# Here is stemmed words
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, lemmatize_words)
corpus = tm_map(corpus, stripWhitespace)
# Creating bag of words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.99999)
dtm
# Import dataset
dataset = read.delim('Restaurant_Reviews.tsv', quote = "",
stringsAsFactors = FALSE )
# Cleaning the texts
# install.packages('tm')
library(tm)
library(SnowballC)
library(textstem)
# Construct the corpus
corpus = VCorpus(VectorSource(dataset$Review))
# Putting all in lower case
# Each of the words should be unique
corpus = tm_map(corpus, content_transformer((tolower)))
# Remove numbers from the data
# Indeed helpful
corpus = tm_map(corpus, removeNumbers)
## Remove puntuation
corpus = tm_map(corpus, removePunctuation)
## Remove stopwords
corpus = tm_map(corpus, removeWords, stpword)
# Here is stemmed words
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, lemmatize_words)
corpus = tm_map(corpus, stripWhitespace)
# Creating bag of words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.99)
dtm
data.frame(dtm)
as.data.frame(dtm)
as.data.frame(as.matrix(dtm))
# Get independent and dependent variables
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
# Import dataset
dataset = read.delim('Restaurant_Reviews.tsv', quote = "",
stringsAsFactors = FALSE )
# Cleaning the texts
# install.packages('tm')
library(tm)
library(SnowballC)
library(textstem)
# Construct the corpus
corpus = VCorpus(VectorSource(dataset$Review))
# Putting all in lower case
# Each of the words should be unique
corpus = tm_map(corpus, content_transformer((tolower)))
# Remove numbers from the data
# Indeed helpful
corpus = tm_map(corpus, removeNumbers)
## Remove puntuation
corpus = tm_map(corpus, removePunctuation)
## Remove stopwords
corpus = tm_map(corpus, removeWords, stpword)
# Here is stemmed words
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, lemmatize_words)
corpus = tm_map(corpus, stripWhitespace)
# Creating bag of words
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
# Build classification model
library(randomForest)
library(caTools)
set.seed(123)
# Get independent and dependent variables
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
# Import dataset
dataset_orig = read.delim('Restaurant_Reviews.tsv', quote = "",
stringsAsFactors = FALSE )
dataset$Liked = dataset_orig$Liked
View(dataset)
dataset
class(dataset$Liked)
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
class(dataset$Liked)
split = sample.split(dataset$Liked, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
clf = randomForest(x = training_set[-767],
y = training_set$Liked,
ntree = 1000)
# Predicting the model
ypred = predict(clf, newdata = test_set[-767])
# Making confusion matrix
cm = table(test_set[,767], ypred)
library(ggplot2)
ggplot(data =  data.frame(cm), mapping = aes(x = ypred, y = Var1))+
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1)+
scale_fill_gradient(low = "lightblue", high = "blue")+
theme_bw() + theme(legend.position = "none")
(107+95)/(107+95+30+18)
test_set[,767]
test_set[,-767]
